{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Audio Bounding Box Annotation Demo\n",
        "\n",
        "This notebook demonstrates how to:\n",
        "1. Load an audio file using OpenSoundscape\n",
        "2. Create a spectrogram and extract time/frequency axes\n",
        "3. Save the spectrogram as a PNG image\n",
        "4. Use jupyter-bbox-widget to draw bounding boxes\n",
        "5. Convert annotations to a DataFrame with audio coordinates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/lviotti/Applications/anaconda3/envs/opso012/lib/python3.10/site-packages/traitlets/traitlets.py:3615: DeprecationWarning: Traits should be given as instances, not types (for example, `Int()`, not `Int`). Passing types is deprecated in traitlets 4.1.\n",
            "  super().__init__(trait=trait, default_value=default_value, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "sys.path.append(\".\")\n",
        "\n",
        "from audio_bbox_annotator import AudioBBoxAnnotator\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Define Parameters\n",
        "\n",
        "Set up the audio file path, time range, and annotation classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Audio file: /Users/lviotti/Library/CloudStorage/Dropbox/Work/Kitzes/datasets/kipu2025/2MM01186/Data/2MM01186_20250126_080003.wav\n",
            "Time range: 315s - 320s\n",
            "Duration: 5s\n",
            "Bandpass: 1 - 10000 Hz\n",
            "Classes: ['a', 'b', 'c']\n"
          ]
        }
      ],
      "source": [
        "# Audio file parameters\n",
        "audio_path = \"/Users/lviotti/Library/CloudStorage/Dropbox/Work/Kitzes/datasets/kipu2025/2MM01186/Data/2MM01186_20250126_080003.wav\"\n",
        "st_time = 315\n",
        "end_time = 320\n",
        "bandpass = [1, 10000]\n",
        "\n",
        "# Annotation classes\n",
        "classes = [\"a\", \"b\", \"c\"]\n",
        "\n",
        "print(f\"Audio file: {audio_path}\")\n",
        "print(f\"Time range: {st_time}s - {end_time}s\")\n",
        "print(f\"Duration: {end_time - st_time}s\")\n",
        "print(f\"Bandpass: {bandpass[0]} - {bandpass[1]} Hz\")\n",
        "print(f\"Classes: {classes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Create and Run the Annotation Pipeline\n",
        "\n",
        "This will:\n",
        "- Load the audio file\n",
        "- Create a spectrogram\n",
        "- Save it as a PNG image\n",
        "- Create the bounding box widget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Starting Audio BBox Annotation Pipeline ===\n",
            "Loading audio from /Users/lviotti/Library/CloudStorage/Dropbox/Work/Kitzes/datasets/kipu2025/2MM01186/Data/2MM01186_20250126_080003.wav\n",
            "Time range: 315s - 320s\n",
            "Duration: 5s\n",
            "Bandpass: 1 - 10000 Hz\n",
            "Audio loaded successfully. Sample rate: 24000 Hz\n",
            "Creating spectrogram...\n",
            "Spectrogram created with shape: (257, 467)\n",
            "Time axis: 0.01s - 4.98s\n",
            "Frequency axis: 0.0 - 12000.0 Hz\n",
            "Saving spectrogram to: temp_spectrogram.png\n",
            "Image saved with dimensions: 1127 x 790 pixels\n",
            "Loading image from: temp_spectrogram.png\n",
            "Image loaded with dimensions: 1127 x 790 pixels\n",
            "Creating bounding box widget...\n",
            "Available classes: ['a', 'b', 'c']\n",
            "BBox widget created successfully!\n",
            "Instructions:\n",
            "1. Click and drag to draw bounding boxes\n",
            "2. Select a class from the dropdown for each box\n",
            "3. Use the widget controls to manage annotations\n",
            "=== Pipeline Complete ===\n",
            "Use get_annotations_dataframe() to get the final results\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/lviotti/GitHub/tdl-notebook/exp/audio_bbox_annotator.py:128: ResourceWarning: unclosed file <_io.BufferedReader name='temp_spectrogram.png'>\n",
            "  self.image = Image.open(self.image_path)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "100d545dd4e943c8b460a055a5d6eeb4",
              "version_major": 2,
              "version_minor": 1
            },
            "text/plain": [
              "BBoxWidget(classes=['a', 'b', 'c'], colors=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create the annotator\n",
        "annotator = AudioBBoxAnnotator(\n",
        "    audio_path=audio_path,\n",
        "    st_time=st_time,\n",
        "    end_time=end_time,\n",
        "    bandpass=bandpass,\n",
        "    classes=classes\n",
        ")\n",
        "\n",
        "# Run the full pipeline\n",
        "bbox_widget = annotator.run_full_pipeline(\n",
        "    output_path=\"temp_spectrogram.png\",  # Save in current directory\n",
        "    dpi=100,\n",
        "    figsize=(12, 8)\n",
        ")\n",
        "\n",
        "# Display the widget\n",
        "display(bbox_widget)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Instructions for Annotation\n",
        "\n",
        "Now you can annotate the spectrogram:\n",
        "\n",
        "1. **Draw bounding boxes**: Click and drag on the image to create bounding boxes\n",
        "2. **Select classes**: Use the dropdown menu to assign a class (\"a\", \"b\", or \"c\") to each box\n",
        "3. **Manage annotations**: Use the widget controls to edit or delete boxes\n",
        "4. **Get results**: Run the cell below to extract the annotations as a DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Extract Annotations\n",
        "\n",
        "After you have drawn your bounding boxes, run this cell to get the results as a DataFrame with audio coordinates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'BBoxWidget' object has no attribute 'get_annotations'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n",
            "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get annotations as DataFrame\u001b[39;00m\n",
            "\u001b[0;32m----> 2\u001b[0m annotations_df \u001b[38;5;241m=\u001b[39m \u001b[43mannotator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_annotations_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Annotations DataFrame ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;32m      5\u001b[0m display(annotations_df)\n",
            "\n",
            "File \u001b[0;32m~/GitHub/tdl-notebook/exp/audio_bbox_annotator.py:155\u001b[0m, in \u001b[0;36mAudioBBoxAnnotator.get_annotations_dataframe\u001b[0;34m(self)\u001b[0m\n",
            "\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBBox widget not created. Run create_bbox_widget() first.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# Get annotations from widget\u001b[39;00m\n",
            "\u001b[0;32m--> 155\u001b[0m annotations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbbox_widget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_annotations\u001b[49m()\n",
            "\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m annotations:\n",
            "\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo annotations found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'BBoxWidget' object has no attribute 'get_annotations'"
          ]
        }
      ],
      "source": [
        "# Get annotations as DataFrame\n",
        "annotations_df = annotator.get_annotations_dataframe()\n",
        "\n",
        "print(\"\\n=== Annotations DataFrame ===\")\n",
        "display(annotations_df)\n",
        "\n",
        "if not annotations_df.empty:\n",
        "    print(\"\\n=== Summary ===\")\n",
        "    print(f\"Total annotations: {len(annotations_df)}\")\n",
        "    print(\"\\nClass distribution:\")\n",
        "    print(annotations_df[\"class\"].value_counts())\n",
        "    print(\"\\n=== Time ranges ===\")\n",
        "    for i, row in annotations_df.iterrows():\n",
        "        print(f\"Box {i+1}: {row['st_time']:.2f}s - {row['end_time']:.2f}s \"\n",
        "              f\"({row['end_time'] - row['st_time']:.2f}s duration)\")\n",
        "    print(\"\\n=== Frequency ranges ===\")\n",
        "    for i, row in annotations_df.iterrows():\n",
        "        print(f\"Box {i+1}: {row['min_freq']:.1f} - {row['max_freq']:.1f} Hz \"\n",
        "              f\"({row['max_freq'] - row['min_freq']:.1f} Hz bandwidth)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Save Results (Optional)\n",
        "\n",
        "Save the annotations to a CSV file for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save annotations to CSV\n",
        "if not annotations_df.empty:\n",
        "    output_file = \"audio_annotations.csv\"\n",
        "    annotations_df.to_csv(output_file, index=False)\n",
        "    print(f\"Annotations saved to: {output_file}\")\n",
        "else:\n",
        "    print(\"No annotations to save.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Verification\n",
        "\n",
        "Let us verify that the coordinate conversion is working correctly by checking the spectrogram properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display spectrogram properties\n",
        "print(\"=== Spectrogram Properties ===\")\n",
        "print(f\"Time axis range: {annotator.time_axis[0]:.2f}s - {annotator.time_axis[-1]:.2f}s\")\n",
        "print(f\"Frequency axis range: {annotator.freq_axis[0]:.1f} - {annotator.freq_axis[-1]:.1f} Hz\")\n",
        "print(f\"Image dimensions: {annotator.image_width} x {annotator.image_height} pixels\")\n",
        "print(f\"Spectrogram shape: {annotator.spec_data.shape}\")\n",
        "\n",
        "# Show a sample of the time and frequency axes\n",
        "print(\"\\n=== Sample Time Axis (first 5 values) ===\")\n",
        "print(annotator.time_axis[:5])\n",
        "\n",
        "print(\"\\n=== Sample Frequency Axis (first 5 values) ===\")\n",
        "print(annotator.freq_axis[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook successfully:\n",
        "\n",
        "1. ✅ **Loaded audio file** using OpenSoundscape with bandpass filtering\n",
        "2. ✅ **Created spectrogram** and extracted time/frequency axes\n",
        "3. ✅ **Saved spectrogram** as PNG with known dimensions\n",
        "4. ✅ **Loaded the image** for annotation\n",
        "5. ✅ **Created bounding box widget** with predefined classes\n",
        "6. ✅ **Generated DataFrame** with columns: \n",
        "\n",
        "The coordinate conversion ensures that the bounding box coordinates in the image are properly converted to the original audio time and frequency scales."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "opso012",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
